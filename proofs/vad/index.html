<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>Proof : Voice detection using microphone</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
</head>
<body>

	<div id="voice" class="jp-jplayer"></div>

	<canvas id="spectrum" width="1275" height="255" style="display: block; border: 1px solid #000;"></canvas>
	<canvas id="energy" width="1275" height="255" style="display: block; border: 1px solid #000;"></canvas>

<script type="text/javascript" src="vad.js"></script>
<script type="text/javascript">

	var context = new (window.AudioContext || window.webkitAudioContext)();
	var microphone;
	var analyser;
	var javascriptNode;

	var canvas_width = 1275; // 5 * 255
	var canvas_height = 255;
	// get the context from the canvas to draw on
	var ctx_spectrum = document.getElementById("spectrum").getContext("2d");
	var ctx_energy = document.getElementById("energy").getContext("2d");

	// create a gradient for the fill. Note the strange
	// offset, since the gradient is calculated based on
	// the canvas, not the specific element we draw
	var gradient = ctx_spectrum.createLinearGradient(0,0,0,canvas_height);
	gradient.addColorStop(1,'#000000');
	gradient.addColorStop(0.75,'#00cc00');
	gradient.addColorStop(0.5,'#cccc00');
	gradient.addColorStop(0.1,'#ff0000');

	function gotAudio(stream) {

		microphone = context.createMediaStreamSource(stream);

		// setup a javascript node
		javascriptNode = context.createScriptProcessor(2048, 1, 1);
		// connect to destination, else it isn't called
		javascriptNode.connect(context.destination);

		// setup an analyzer
		analyser = context.createAnalyser();
		analyser.smoothingTimeConstant = 0.8; // 0.3;
		analyser.fftSize = 512;

		// analyser.maxDecibels = -20;
		// analyser.minDecibels = -60;

		// connect up the nodes
		microphone.connect(analyser);
		analyser.connect(javascriptNode);

		myVad = new vad(analyser);

		// when the javascript node is called
		// we use information from the analyzer node
		// to draw the volume
		javascriptNode.onaudioprocess = function() {

			// get the average for the first channel
			var array = new Uint8Array(analyser.frequencyBinCount);
			analyser.getByteFrequencyData(array);
			drawSpectrum(ctx_spectrum, array);

			drawSpectrum(ctx_energy, [
				myVad.getEnergy(),
				255*myVad.getFrequency()/12000, // Normalized the frequency so 12kHz is max on graph
				-25.5*myVad.getSFM()
			]);
		};

		// TMP - Remove to avoid feedback!
		// microphone.connect(context.destination);
	}

	function logError(error) {
		console.log(error.name + ": " + error.message);
	}

	// console.dir(navigator);
	// navigator.getUserMedia('audio', gotAudio, logError);

	if(navigator.getUserMedia) { // W3C
		navigator.getUserMedia({audio:true}, gotAudio, logError);
	} else if(navigator.webkitGetUserMedia) { // WebKit
		navigator.webkitGetUserMedia({audio:true}, gotAudio, logError);
	} else if(navigator.mozGetUserMedia) { // Mozilla
		navigator.mozGetUserMedia({audio:true}, gotAudio, logError);
	}

	function drawSpectrum(ctx, array) {

		// clear the current state
		ctx.clearRect(0, 0, canvas_width, canvas_height);

		// set the fill style
		ctx.fillStyle = gradient;

		for ( var i = 0, iLen = array.length; i < iLen; i++ ){
			var value = array[i];
			ctx.fillRect(i*5, canvas_height - value, 3, value);
		}
	};

	console.log("sample rate: " + context.sampleRate);

</script>
</body>
</html>
